{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Gensim\n",
    "\n",
    "This is a simple worked example for training and interpreting a topic model with [Gensim](https://radimrehurek.com/gensim). It assumes that the input text collection has already been cleaned and processed into a single text file, with one document per line, and each document being represented as a whitespaced sperated tokens. For a more in-depth example that also includes using [Spacy](https://spacy.io) for text cleaning, see Patrick Harrison's [Modern NLP in Python](https://www.youtube.com/watch?v=6zm9NC9uRkk) talk and accompanying [notebook](https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "To go through this tutorial, first create a virtual environment and then install the needed packages into it with:\n",
    "\n",
    "    $ pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# this file should contain a collection of documents, one per line,\n",
    "# with each document being represented as whitespace-separated tokens\n",
    "TEXT_FILE_PATH = \"cleaned_text.txt\"\n",
    "\n",
    "# The number of worker processes to use for parallel computing. A good number \n",
    "# is N-1, where N is the number of actual CPU cores you have (not hyperthreads). \n",
    "# You might need to set this to 1 on Windows.\n",
    "WORKERS = 3\n",
    "\n",
    "def tokens_from_file(path):\n",
    "    \"\"\"Generator yielding tokens for each document in a file.\n",
    "    Assumes input file has one document per line with tokens separated \n",
    "    by whitespace. \n",
    "    \"\"\"\n",
    "    with open(path, encoding=\"utf8\") as file:\n",
    "        for line in file:\n",
    "            yield line.strip().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an LDA Topic Model\n",
    "First we need to build a dictionary from the cleaned text. Gensim provides a `Dictionary` class that efficiently stores the vocabulary you will be using from your collection. As part of this process we filter out tokens that are very rare or very common from the corpus. With these settings below, a token must occur at least 10 times to be kept, and it will be filtered out if it occurs in more than 40% of documents. See [Gensim's documentation](https://radimrehurek.com/gensim/corpora/dictionary.html) for other parameters to the `filter_extremes` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary has 181965 documents and 22475719 tokens in its vocabulary\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "# crate the dictionary with cleaned tokens\n",
    "dictionary = Dictionary(tokens_from_file(TEXT_FILE_PATH))\n",
    "\n",
    "# filter rare and common tokens from the dictionary\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "\n",
    "# this removes gaps in the dictionary's data structure after token filtering, saving some space  \n",
    "dictionary.compactify()\n",
    "\n",
    "print(f\"The dictionary has {dictionary.num_docs} documents and {dictionary.num_pos} tokens in its vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to use the trained dictionary model to convert the corpus into a sequence of bag of words, before we can train the LDA model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 9), (1, 1), (2, 4), (3, 1), (4, 1), (5, 9), (6, 1), (7, 5), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 3), (17, 1), (18, 7), (19, 1)]\n"
     ]
    }
   ],
   "source": [
    "bow_texts = [dictionary.doc2bow(tokens) for tokens in tokens_from_file(TEXT_FILE_PATH)]\n",
    "\n",
    "# show the first 20 tokens of the first document\n",
    "print(bow_texts[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our collection is now represented as a list of documents, with each document being represented as a list of tokens. Note that in this representation used by the dictionary, tokens have been converted into integer IDs, which is a more efficient representation. This means that whenever we need to get back the original token representation, we'll need the dictionary to convert the token IDs back to their character representations.\n",
    "\n",
    "Now train the LDA model. We'll use Gensim's [LdaMulticore](https://radimrehurek.com/gensim/models/ldamulticore.html) class to perform parallel training using multiple worker processes, which will make the training faster. If you don't have access to an environment/machine with multicore support, set `workers` to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "\n",
    "lda = LdaMulticore(\n",
    "    bow_texts,\n",
    "    num_topics=15,\n",
    "    id2word=dictionary,\n",
    "    workers=WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Model\n",
    "\n",
    "Let's write a helper function to explore the top terms for each topic. Have a look at a few different topics and see if you can identify a coherent theme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token                probability\n",
      "\n",
      "government           0.019\n",
      "australia            0.013\n",
      "-d-                  0.011\n",
      "year                 0.008\n",
      "industry             0.007\n",
      "australian           0.007\n",
      "need                 0.007\n",
      "climate_change       0.007\n",
      "job                  0.007\n",
      "-year-               0.007\n",
      "policy               0.006\n",
      "economy              0.006\n",
      "cost                 0.006\n",
      "support              0.006\n",
      "scheme               0.006\n",
      "labor                0.006\n",
      "country              0.005\n",
      "people               0.005\n",
      "-dd-_cent            0.005\n",
      "business             0.005\n",
      "future               0.005\n",
      "increase             0.004\n",
      "work                 0.004\n",
      "sector               0.004\n",
      "coalition            0.004\n"
     ]
    }
   ],
   "source": [
    "def explore_topic(model, topic_number, topn=20):\n",
    "    \"\"\"Accept a user-supplied topic number and\n",
    "    print out a formatted list of the top tokens by probability\n",
    "    \"\"\"\n",
    "    print(f\"{'token':20} probability\\n\")\n",
    "    for token, probability in model.show_topic(topic_number, topn=25):\n",
    "        print(f\"{token:20} {probability:.3f}\")\n",
    "\n",
    "\n",
    "explore_topic(lda, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While looking at the top key words is useful, an even better approach is the [pyLDAvis](https://github.com/bmabey/pyLDAvis) tool. Note that this can take a while to run due to the dimensionality reduction it has to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ned/.pyenv/versions/3.7.3/envs/tm-demo/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "ldavis_prepared = pyLDAvis.gensim.prepare(\n",
    "    lda, \n",
    "    bow_texts, \n",
    "    dictionary, \n",
    "    sort_topics=False, \n",
    "    n_jobs=WORKERS\n",
    ")\n",
    "\n",
    "pyLDAvis.display(ldavis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "\n",
    "def train_lda_models(end, start=2, step=5, coherence=\"u_mass\"):\n",
    "    \"\"\"Train LDA models for a range of topic numbers and get coherence scores\n",
    "    \n",
    "    end:       max number of topics to test\n",
    "    start:     min number of topics to test\n",
    "    step:      increments in topic number range to test\n",
    "    coherence: coherence metric to be used. Must be valid value for the 'coherence'\n",
    "               param of Gensim's 'CoherenceModel'.\n",
    "               \n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for num_topics in range(start, end+1, step):\n",
    "        print(f\"Training topic model with {num_topics} topics...\")\n",
    "        model = LdaMulticore(\n",
    "            bow_texts,\n",
    "            num_topics=num_topics,\n",
    "            id2word=dictionary,\n",
    "            workers=WORKERS,\n",
    "        )\n",
    "        cm = CoherenceModel(\n",
    "            model=model, \n",
    "            corpus=bow_texts, \n",
    "            texts=tokens_from_file(TEXT_FILE_PATH), \n",
    "            coherence=coherence\n",
    "        )\n",
    "    results.append(num_topics, model, cm.get_coherence())\n",
    "\n",
    "    \n",
    "model_results = train_lda_models(end, start=2, step=5)\n",
    "\n",
    "# TODO: show graphic the results and also describe what's going on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Topic Model\n",
    "\n",
    "TODO: finish this\n",
    "Once we've settled on our trained a topic model which is showing us topics that seem coherent..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
